
#' Extract cells located in regions exceeding random background expression level
#'
#' \code{get_sigcells} extracts an indicator matrix (cells x genes) specifying which cells are located in regions with higher expression than can be expected by chance, given the spatial location of the cells. It takes the output from \code{cellsceek_test} as input.
#'
#' @param cellpeak_stats A list with with cell-peak statistics for each cell and gene as output by \code{cellsceek_test}.
#' 
#' @return An indicator matrix (cells x genes) containing 0's and 1's specifying which cells are located in regions which significantly higher expression.
#'
#' @examples
#' pp = sim_pois(100)
#' low_expr = c(10, 10)
#' high_expr = c(15, 20)
#' pp = add_markdist_streak(pp, low_expr, high_expr)
#' cellpeak_stats = cellsceek_test(pp)
#' cell_ind_mat = get_sigcells(cellpeak_stats)
#' 
#' @export
get_sigcells <- function(cellpeak_stats){
  gene2sigcells_list = lapply(cellpeak_stats, function(j_gene){j_gene_ordered = dplyr::arrange_(j_gene, 'cell'); dplyr::select_(j_gene_ordered, 'sig.bin')})
  sigcells_df = dplyr::bind_cols(gene2sigcells_list)
  colnames(sigcells_df) = names(cellpeak_stats)
  rownames(sigcells_df) = dplyr::arrange_(cellpeak_stats[[1]], 'cell')$cell
  
  return(sigcells_df)
}

#' Identify cells located in regions exceeding random background expression level
#'
#' \code{cellsceek_test} identifies cells located in regions with higher expression level than expected by random. The spatial distribution is presumed to be fixed and conditioned on that, the test assesses whether cells are in a region with high expression levels. The background expression 2-dimensional null-distribution is generated by random resampling of the mark distribution followed by 2-dimensional kernel density estimate for each resampling.
#'
#' @param pp A point pattern with one or more mark distributions.
#' @param nrand An integer specifying the number of random resamplings of the mark distribution as to create the null-distribution.
#' @param cell_alpha A numeric specifying a signficance level which is used to flag if a cell has significantly higher expression than expected by random for a particular gene or not.
#' @param h A numeric vector of length 2 specifying the bandwidth of the two-dimensional Gaussian kernel (x, y).
#' 
#' @return A list containing statistics for all cells for each gene.
#'
#' @examples
#' pp = sim_pois(100)
#' low_expr = c(10, 10)
#' high_expr = c(15, 20)
#' pp = add_markdist_streak(pp, low_expr, high_expr)
#' cellpeaks = cellsceek_test(pp)
#' 
#' @export
cellsceek_test <- function(pp, nrand = 100, cell_alpha = 0.05, h = NA){
  
  ##get cell-stats
  cellstats_list = list()
  marx = pp[['marks']]
  if(!is.numeric(marx) || is.matrix(marx)){
    genes = colnames(pp[['marks']])
  }else{
    genes = 'g1'
  }
  
  for(j_gene in genes){
    print(j_gene)
    j_pp = pp_select(pp, j_gene)
    
    ##get cellstats
    cellstats_list[[j_gene]] = cellsceek_jpp(j_pp, nrand = nrand, cell_alpha = cell_alpha, h = h)
  }
  
  return(cellstats_list)
}

cellsceek_jpp <- function(j_pp, nrand = 1000, cell_alpha = 0.05, h = NA){
  ###NB: assumes that the marks are log10-transformed
  
  pos_mat = cbind(j_pp[['x']], j_pp[['y']])
  colnames(pos_mat) = c('x', 'y')
  marx = j_pp[['marks']]
  if(is.data.frame(marx) || is.matrix(marx)){
    if(ncol(marx) > 1){
      stop('The marks can only be of of one-dimension (a single gene)')
    }else{
      marx_vec = marx[, 1]
      names(marx_vec) = rownames(marx)
      marx = marx_vec
    }
  }
  
  ##observed kde
  weights = round(10^marx) ##weights: a vector of integers representing frequencies of individual observations.
  if(is.logical(h)){
    obs.dens = sm::sm.density(x = pos_mat, weights = weights, nbins = 0, display = 'none')
  }else{
    obs.dens = sm::sm.density(x = pos_mat, weights = weights, nbins = 0, display = 'none', h = h)
  }
  est.obs = obs.dens$estimate
  
  ##generate kde null-dist by random sampling of marx (weights)
  eval.points = obs.dens$eval.points
  npoints_x = nrow(eval.points)
  est.rand = array(NA, dim = c(npoints_x, npoints_x, nrand))
  for(jrand in 1:nrand){
    weights.rand = sample(weights)
    if(is.logical(h)){
      rand.dens = sm::sm.density(x = pos_mat, weights = weights.rand, nbins = 0, eval.points = eval.points, display = 'none')
    }else{
      rand.dens = sm::sm.density(x = pos_mat, weights = weights.rand, nbins = 0, eval.points = eval.points, display = 'none', h = h)
    }
    est.rand[, , jrand] = rand.dens$estimate
  }
  
  ##upper kde null-dist boundary
  upper = apply(est.rand, c(1, 2), stats::quantile, probs = 1 - cell_alpha)
  
  ##group grid-bins on below or above upper boundary
  dim(est.obs) = c(npoints_x^2, 1)
  dim(upper) = c(npoints_x^2, 1)
  est.df = as.data.frame(cbind(est.obs, upper))
  colnames(est.df) = c('obs', 'upper')
  est.df = dplyr::mutate_(est.df, sig.bin = ~ifelse(obs > upper, 1, 0), bin.id = ~row_number())
  
  ##assign each cell (pos_mat) to its grid-bin
  bin.id = apply(pos_mat, 1, function(j_pos){
    j.x = which.min(abs(j_pos['x'] - eval.points[, 'xnew'])) ##xgrid
    j.y = which.min(abs(j_pos['y'] - eval.points[, 'ynew'])) ##ygrid
    jbin = j.x + (j.y - 1) * 50
  })
  cell_df = as.data.frame(cbind(pos_mat, marx, bin.id))
  if(is.null(names(marx))){
    names(marx) = paste('p', 1:length(marx), sep = '')
  }
  cell_df = dplyr::bind_cols(cell_df, as.data.frame(names(marx), stringsAsFactors = FALSE))
  colnames(cell_df)[ncol(cell_df)] = 'cell'
  
  ##join cell2gridbin and gridbin2sigbin
  cell_df = dplyr::inner_join(est.df, cell_df, by = 'bin.id')    
  
  return(cell_df)
}


#' Test for the presence of spatial expression patterns
#'
#' \code{trendsceek_test} tests for the presence of spatial expression patterns. The spatial distribution is presumed to be fixed and conditioned on that, the test assesses whether the mark distribution is non-random.
#'
#' @param pp A point pattern with one or more mark distributions.
#' @param nrand An integer specifying the number of random resamplings of the mark distribution as to create the null-distribution.
#' @param ncores An integer specifying the number of cores to be used by BiocParallel
#' @param alpha_env A numeric specifying a Bonferroni-significance level used to create a test-statistic threshold for each radius, an "envelope". Note that this is solely used for plotting purposes.
#' @param alpha_bh A numeric specifying a Benjamini-Hochberg signficance level used to extract significant genes (note that this can also be done for an arbitrary alpha after trendsceek_test has been called, see \code{extract_sig_genes}).
#' @param alpha_nom_early A numeric specifying an early-stopping threshold for the p-value. If the nominal p-value exceeds this value no more permutations are done for that gene.
#' 
#' @return A list containing trendsceek-statistics for every gene.
#'
#' @examples
#' pp = sim_pois(100)
#' low_expr = c(10, 10)
#' high_expr = c(15, 20)
#' pp = add_markdist_hotspot(pp, low_expr, high_expr)
#' trendstat_list = trendsceek_test(pp, nrand = 100, ncores = 1)
#' 
#' @export
trendsceek_test <- function(pp, nrand = 1e4, ncores = 1, alpha_env = 0.1 / ifelse(is.numeric(pp[['marks']]), length(pp[['marks']]), ncol(pp[['marks']])), alpha_bh = 0.05, alpha_nom_early = (alpha_bh * 4) / ifelse(ifelse(is.numeric(pp[['marks']]), length(pp[['marks']]), ncol(pp[['marks']])) >= 500, 10, 1)){
  
  ##init parallelization
  bp_param = BiocParallel::MulticoreParam(workers = ncores)
  
  ##get rstats
  marx = pp[['marks']]
  if(is.numeric(marx)){
    nfeats = 1
    feats = 1
  }else{
    nfeats = ncol(marx)
    feats = colnames(marx)
  }
  tstat_list = BiocParallel::bplapply(1:nfeats, calc_trendstats, BPPARAM = bp_param, pp = pp, n.rand = nrand, alpha_env = alpha_env, alpha_nom_early = alpha_nom_early)
  names(tstat_list) = feats
  
  ##get supinum stats
  supstats_list = tstat2supstat(tstat_list)
  supstats_wide = supstats_list2wide(supstats_list)
  
  ##store
  trendstat_list = list(tstat = tstat_list, supstats = supstats_list, supstats_wide = supstats_wide)
  
  ##get sig genes
  sig_list = extract_sig_genes(trendstat_list, alpha_bh)
  
  # trendstat_list[['sig_genes_list']] = sig_list
  
  return(trendstat_list)
}

trendsceek_test_nonpar <- function(pp, nrand = 1e4, ncores = 1, alpha_env = 0.05 / ifelse(is.numeric(pp[['marks']]), length(pp[['marks']]), ncol(pp[['marks']])), alpha_bh = 0.05){
  
  ##get rstats
  marx = pp[['marks']]
  if(is.numeric(marx)){
    nfeats = 1
    feats = 1
  }else{
    nfeats = ncol(marx)
    feats = colnames(marx)
  }
  tstat_list = lapply(1:nfeats, calc_trendstats, pp = pp, n.rand = nrand, alpha_env = alpha_env)
  names(tstat_list) = feats
  
  ##get supinum stats
  supstats_list = tstat2supstat(tstat_list)
  supstats_wide = supstats_list2wide(supstats_list)
  
  ##store
  trendstat_list = list(tstat = tstat_list, supstats = supstats_list, supstats_wide = supstats_wide)
  
  ##get sig genes
  sig_list = extract_sig_genes(trendstat_list, alpha_bh)
  
  trendstat_list[['sig_genes_list']] = sig_list
  
  return(trendstat_list)
}

#' Extract significant genes from trendsceek results
#'
#' \code{extract_sig_genes} extracts significant genes from trendsceek results.
#'
#' @param trendstat_list A list containing results generated by calling \code{trendsceek_test}.
#' @param alpha A numeric specifying a Benjamini-Hochberg signficance level used to extract significant genes.
#' 
#' @return A list containing significant genes for each test.
#'
#' @examples
#' pp = sim_pois(100)
#' low_expr = c(10, 10)
#' high_expr = c(15, 20)
#' pp = add_markdist_hotspot(pp, low_expr, high_expr)
#' trendstat_list = trendsceek_test(pp, nrand = 100, ncores = 1)
#' sig_list = extract_sig_genes(trendstat_list, alpha = 0.05)
#' 
#' @export
extract_sig_genes <- function(trendstat_list, alpha = 0.05){
  
  supstats_list = trendstat_list[['supstats']]
  
  ##p.BH
  sig.list = lapply(supstats_list, function(j.df){j.df[which(j.df[, 'p.bh'] <= alpha), ]})
  
  ##remove empty
  anysig.test = names(sig.list)[which(unlist(lapply(sig.list, nrow)) > 0)]
  sig.list = sig.list[anysig.test]
  
  return(sig.list)
}

supstats_list2wide <- function(supstats.list){
  
  ##list -> wide table
  stats.long = data.table::melt(supstats.list, id.vars = colnames(supstats.list[[1]])) 
  stats.long = stats.long[, -ncol(stats.long)] #test column already present
  
  if('vargenes.rank' %in% colnames(stats.long)){
    value.vars = setdiff(colnames(stats.long), c('gene', 'vargenes.rank', 'test'))
    stats.wide = as.data.frame(data.table::dcast(data.table::setDT(stats.long), gene + vargenes.rank ~ test, value.var = value.vars))
  }else{
    value.vars = setdiff(colnames(stats.long), c('gene', 'test'))
    stats.wide = as.data.frame(data.table::dcast(data.table::setDT(stats.long), gene ~ test, value.var = value.vars))
  }
  
  ##set rownames
  rownames(stats.wide) = stats.wide[, 'gene']
  
  ##order
  stats.wide = dplyr::arrange_(stats.wide, 'rank_markcorr')
  
  return(stats.wide)
}

tstat2supstat <- function(tstat_list){
  
  ##rm genes where an error occured    
  pass.genes = names(tstat_list)[which(unlist(lapply(lapply(tstat_list, class), length)) == 1)]
  tstat_list = tstat_list[pass.genes]
  
  ##extreme point (supremum/supinum) stats 
  supstats.names = setdiff(names(tstat_list[[1]][[1]]), 'r.stat')
  supstats.list = lapply(tstat_list, function(jgene_list, sup.stats){lapply(jgene_list, function(jtest_list, sup.stats = sup.stats){jtest_list[sup.stats]}, sup.stats = sup.stats)}, sup.stats = supstats.names)
  supstats.long = reshape2::melt(supstats.list)
  colnames(supstats.long) = c('value', 'sup.stat', 'test', 'gene')
  
  ##long -> wide wrt sup.stat
  supstats.df = data.table::dcast(supstats.long, gene + test ~ sup.stat, value.var = 'value')
  
  ##split by test and order genes
  supstats.list = split(supstats.df, supstats.df[, 'test'])
  supstats.list = lapply(supstats.list, function(j.teststat){j.teststat[order(j.teststat[, 'min.pval'], -j.teststat[, 'max.env.rel.dev']), ]})
  
  ##add adjusted pval
  supstats.list = lapply(supstats.list, function(j.teststat){p.bh = stats::p.adjust(j.teststat[, 'min.pval'], method = 'BH'); j.teststat = cbind(j.teststat, p.bh); return(j.teststat);})        
  supstats.list = lapply(supstats.list, function(j.teststat){p.bo = stats::p.adjust(j.teststat[, 'min.pval'], method = 'bonferroni'); j.teststat = cbind(j.teststat, p.bo); return(j.teststat);})        
  
  ##add rank column
  supstats.list = lapply(supstats.list, function(j.teststat){rank = 1:nrow(j.teststat); j.teststat = cbind(j.teststat, rank); return(j.teststat);})
  
  ##set rownames
  supstats.list = lapply(supstats.list, function(jtest.list){rownames(jtest.list) = jtest.list[, 'gene']; return(jtest.list);})
  
  return(supstats.list)
}

calc_trendstats <- function(jfeat_it, pp, n.rand = 1e4, alpha_env = 0.05, alpha_nom_early = (0.05 * 4) / ifelse(ifelse(is.numeric(pp[['marks']]), length(pp[['marks']]), ncol(pp[['marks']])) >= 500, 10, 1), methods = c('Emark', 'markcorr', 'markvario', 'Vmark')){
  
  print(jfeat_it)
  
  ##subset on a single mark
  marx = pp[['marks']]
  if(!is.numeric(marx)){
    pp[['marks']] = marx[, jfeat_it]
  }
  
  ##generate null dist by permuting marks
  ##the same null-dist is used for all methods for comparability
  pp.perm.list = gen_null(pp, n.rand)
  ##TBD: Possibly need to use pool.envelope if running into mem-constraints
  
  ##calc stats
  tstat.list = calc_pp_trendstats(pp, pp.perm.list, alpha_env, alpha_nom_early, methods)
  
  return(tstat.list)
}

gen_null <- function(pp, n.rand = 2000){
  ##Randomize to create null
  ##List of pps with fixed positions but randomized labels
  
  perm = TRUE #keep the marginal mark dist the same
  pp.perm.list = lapply(1:n.rand, function(j.it, pp, permute){spatstat.core::rlabel(pp, permute = permute)}, pp = pp, permute = perm)    
  
  return(pp.perm.list)
}

calc_pp_trendstats <- function(pp, pp.perm.list, alpha_env = 0.05, alpha_nom_early = 0.5, methods = c('Emark', 'markcorr', 'markvario', 'Vmark')){
  ###Calculate test statistic of obs and null
  
  ##available test stat methods
  fcns = c(spatstat.core::Emark, spatstat.core::markcorr, spatstat.core::markvario, spatstat.core::Vmark)
  names(fcns) = c('Emark', 'markcorr', 'markvario', 'Vmark')
  
  ##subset on selected methods
  fcns = fcns[methods]
  
  ##calc stats for each selected method
  nsim = length(pp.perm.list)    
  tstat.list = list()
  for(j.fcn.name in names(fcns)){
    j.fcn = fcns[[j.fcn.name]]
    
    print(j.fcn.name)        
    
    tstat.list[[j.fcn.name]] = calc_pp_trendstats_jmethod(pp, pp.perm.list, j.fcn, alpha_env, alpha_nom_early)
  }
  
  return(tstat.list)
}

calc_pp_trendstats_jmethod <- function(pp, pp.perm.list, j.fcn, alpha_env = 0.05, alpha_nom_early = 0.5){
  ###Test stats for observed and simulated null data
  
  nsim = length(pp.perm.list)    
  
  ##alpha_early = alpha * alpha_earlystop_mult. Increase nsim one order of magnitude at a time, e.g. 10, 100, 1000. 
  nsim_order = floor(log10(nsim))
  nsim_vec = 10^(1:nsim_order)
  if(nsim != nsim_vec[length(nsim_vec)]){
    nsim_vec = c(nsim_vec, nsim)
  }
  
  ##loop over increasing nsim
  n_simsplits = length(nsim_vec)
  local.stats.list = list()
  j_nsim_it = 0
  j_p = 0
  alpha_nom_earlystop = 0.5 ## just used for 10 first permutations
  tstat.list = list()
  while(j_p <= alpha_nom_earlystop && j_nsim_it < n_simsplits){
    j_nsim_it = j_nsim_it + 1
    if(j_nsim_it > 1){
      alpha_nom_earlystop = alpha_nom_early
    }
    
    ##select subset from pregenerated null (pp.perm.list)
    ##e.g. 11-100
    if(j_nsim_it == 1){
      jperm_start = 1
    }else{
      jperm_start = nsim_vec[j_nsim_it - 1] + 1
    }            
    jperm_end = nsim_vec[j_nsim_it]
    
    j_pp_perm_list = pp.perm.list[jperm_start:jperm_end]
    
    
    ##get local (per radius) test stats for subset
    ##This is the time-consuming step
    j_nsim = length(j_pp_perm_list)
    j_localstats = spatstat.core::envelope(pp, j.fcn, nsim = j_nsim, simulate = j_pp_perm_list, global = FALSE, nsim2 = 0, savefuns = TRUE, verbose = FALSE)
    
    ##add test-stats from previous subsets
    local.stats.list = add_subsetstats(local.stats.list, j_localstats, j_nsim_it)
    
    ##get point-wise envelope test-stats and p-value
    j_envstats.df = get_envstats(local.stats.list[['tstat']], local.stats.list[['nullstats']], alpha_env) ##tstat.local.df, nullstats.local.df
    
    ##global envelope p-value (min across all radii)
    j_p = min(j_envstats.df[, 'p'])
  }
  
  if(j_nsim_it < n_simsplits){
    earlystop = 1
  }else{
    earlystop = 0
  }
  
  ##*##    
  ##store stats in list
  ##*##
  
  ##pointwise stats: for every radius value
  tstat.list[['r.stat']] = cbind(local.stats.list[['tstat']], j_envstats.df)
  
  ##global envelope stats
  tstat.list[['max.env.rel.dev']] = max(j_envstats.df[, 'env.rel.dev'])
  tstat.list[['max.rel.dev']] = max(j_envstats.df[, 'mean.rel.dev'])
  tstat.list[['min.pval']] = j_p
  tstat.list[['earlystop']] = earlystop
  tstat.list[['nsim_stop']] = j_nsim_it
  tstat.list[['nsim_max']] = n_simsplits
  
  return(tstat.list)
}

get_envstats <- function(tstat.df, nullstats.df, alpha_env){
  
  ##test-stat exp value can differ between different radii, therefore look at deviation (distance) from the exp and not the test-stat directly
  null.mean = tstat.df[, 'exp']
  null.devs = apply(nullstats.df, 2, function(j.sim, null.mean){abs(j.sim - null.mean)}, null.mean = null.mean)
  ##TBD: variance is scale-dependent, varying with exp, which one may want to account for
  
  null.sorted = t(apply(null.devs, 1, sort)) #sort every r_i
  null.global = sort(apply(null.sorted, 2, max), decreasing = TRUE) #max of every kth ranked value across all r_i
  
  ##*###
  ##P-value calculation
  ##*###
  nsim = ncol(nullstats.df)
  obs.dev = abs(tstat.df[, 'obs'] - null.mean)
  hi.rank = unlist(lapply(obs.dev, function(j.r, null.global){length(which(null.global >= j.r))}, null.global = null.global))
  hi.rank = hi.rank + 1
  p = hi.rank / (nsim + 1) ##no need to use lo.rank and take *2 since already accounted for two-sided test by taking absolute values
  
  ##*###
  ##Global envelope
  ##*###
  ##already a two.sided test since took absolute values above
  kth.nullval = floor((nsim + 1) * alpha_env)
  if(kth.nullval == 0){
    kth.nullval = 1
  }
  max.dev = null.global[kth.nullval]
  
  ##global dev
  hi.global = null.mean + max.dev
  lo.global = null.mean - max.dev
  
  ##Relative deviation from envelope
  env.rel.dev = obs.dev / max.dev
  
  ##*###
  ##Effect size
  ##*###
  ##relative deviation from null.mean
  mean.rel.dev = obs.dev / null.mean
  
  ##Store
  envstats = cbind(obs.dev, lo.global, hi.global, env.rel.dev, mean.rel.dev, p)
  
  return(envstats)
}

add_subsetstats <- function(local.stats.list, j_localstats, j_nsim_it){
  
  j_nullstats.local.df = localstat2nulldf(j_localstats)            
  
  ##add test-stats from previous subsets
  if(j_nsim_it == 1){
    tstat.local.df = localstat2df(j_localstats)
    nullstats.local.df = j_nullstats.local.df
    
  }else{
    
    ##add null stats from previous subset
    nullstats.local.df = local.stats.list[['nullstats']]
    nullstats.local.df = cbind(nullstats.local.df, j_nullstats.local.df)
    
    ##recalculate null-dist based values present in tstat.df, 'r'
    ##and 'obs' are the same regardless of null dist
    tstat.local.df = local.stats.list[['tstat']]
    tstat.local.df[, 'exp'] = apply(nullstats.local.df, 1, mean)
    tstat.local.df[, 'lo.local'] = apply(nullstats.local.df, 1, min)
    tstat.local.df[, 'hi.local'] = apply(nullstats.local.df, 1, max)
  }
  
  local.stats.list = list(tstat = tstat.local.df, nullstats = nullstats.local.df)
  
  return(local.stats.list)    
}

localstat2df <- function(tstat){
  
  ##tstat -> df    
  tstat.df = as.data.frame(tstat)
  cols = colnames(tstat.df)
  cols[which(cols == 'mmean')] = 'exp'
  cols[which(cols == 'lo')] = 'lo.local'
  cols[which(cols == 'hi')] = 'hi.local'
  colnames(tstat.df) = cols
  
  return(tstat.df)
}

localstat2nulldf <- function(tstat){
  ###get null stats
  
  nullstats.df = as.data.frame(attr(tstat, 'simfuns'))
  
  ##exclude radii
  nullstats.df.nor = nullstats.df[, setdiff(colnames(nullstats.df), 'r')]
  
  return(nullstats.df.nor)
}

pos2pp <- function(pos_mat){
  x.range = range(pos_mat[, 1])
  y.range = range(pos_mat[, 2])
  pp = spatstat.geom::ppp(x = pos_mat[, 1], y = pos_mat[, 2], x.range, y.range)
  
  return(pp)
}


deseq_norm <- function(counts, min.count){
  
  ##create deseq dataset
  colData = as.data.frame(colnames(counts), stringsAsFactors = FALSE)
  colnames(colData) = 'cellName'
  dds = DESeq2::DESeqDataSetFromMatrix(countData = counts, colData = colData, design = ~ 1)
  
  ##filter genes on being expressed
  dds = dds[ rowSums(DESeq2::counts(dds)) > min.count, ]
  
  ##estimate size factors
  dds = DESeq2::estimateSizeFactors(dds,type = "poscounts")
  ##size.factors = DESeq2::sizeFactors(dds)
  
  ##divide with size factor
  counts_norm = DESeq2::counts(dds, normalized = TRUE)
  
  return(counts_norm)
}


